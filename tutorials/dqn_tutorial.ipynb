{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2023 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Train a Deep Q Network with TF-Agents\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\n",
        "\n",
        "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\n",
        "\n",
        "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\n",
        "\n",
        "\n",
        "To run this code live, click the 'Run in Google Colab' link above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "# !pip install 'imageio==2.4.0'\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install tf-agents\\[reverb\\]\n",
        "# !pip install pyglet\n",
        "# !pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-03 14:48:24.007060: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-03 14:48:25.461375: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-03 14:48:31.183473: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-03 14:48:31.183577: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-03 14:48:31.231147: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-03 14:48:34.412143: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-03 14:48:34.430351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-03 14:48:42.433087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "outputs": [],
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NspmzG4nP3b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.14.1'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [],
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "Load the CartPole environment from the OpenAI Gym suite. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n"
          ]
        }
      ],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ],
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bttJ4uxZUQBr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
          ]
        }
      ],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "In the Cartpole environment:\n",
        "\n",
        "-   `observation` is an array of 4 floats: \n",
        "    -   the position and velocity of the cart\n",
        "    -   the angular position and velocity of the pole \n",
        "-   `reward` is a scalar float value\n",
        "-   `action` is a scalar integer with only two possible values:\n",
        "    -   `0` — \"move left\"\n",
        "    -   `1` — \"move right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V2UGR5t_iZX-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time step:\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 0.0337444 , -0.02617371,  0.04356907, -0.04016002], dtype=float32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'step_type': array(0, dtype=int32)})\n",
            "Next time step:\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 0.03322093,  0.16829726,  0.04276587, -0.31878442], dtype=float32),\n",
            " 'reward': array(1., dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n"
          ]
        }
      ],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually two environments are instantiated: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "outputs": [],
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
        "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290)\n",
        "\n",
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "We will use `tf_agents.networks.` to create a `QNetwork`. The network will consist of a sequence of `tf.keras.layers.Dense` layers, where the final layer will have 1 output for each possible action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "# q_values_layer = tf.keras.layers.Dense(\n",
        "#     num_actions,\n",
        "#     activation=None,\n",
        "#     kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "#         minval=-0.03, maxval=0.03),\n",
        "#     bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "actor_values_layer = tf.keras.layers.Dense(\n",
        "    1,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "v_values_layer = tf.keras.layers.Dense(\n",
        "    1,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "a_net = sequential.Sequential(dense_layers + [actor_values_layer,\n",
        "                                            tf.keras.layers.Activation('sigmoid'),\n",
        "                                            tf.keras.layers.Lambda(lambda x: tf.cast(tf.round(x), dtype=tf.int32))])\n",
        "v_net = sequential.Sequential(dense_layers + [v_values_layer])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "\n",
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(train_py_env))\n",
        "\n",
        "critic_net = critic_network.CriticNetwork(\n",
        "    (observation_spec, action_spec),\n",
        "    observation_fc_layer_params=None,\n",
        "    action_fc_layer_params=None,\n",
        "    joint_fc_layer_params=fc_layer_params,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    last_kernel_initializer='glorot_uniform')\n",
        "\n",
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-03 14:49:10.379058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-03 14:49:13.269472: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
            "<tf_agents.networks.sequential.Sequential object at 0x7f3cf3b44510>\n"
          ]
        }
      ],
      "source": [
        "print(train_env.action_spec())\n",
        "print(a_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "In this tutorial:\n",
        "\n",
        "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
        "-   The policy returns an action (left or right) for each `time_step` observation.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
        "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HE37-UCIrE69"
      },
      "outputs": [],
      "source": [
        "# random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "#                                                 train_env.action_spec())\n",
        "from tf_agents.policies import random_py_policy\n",
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  train_py_env.time_step_spec(), train_py_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
        "\n",
        "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
        "-   `state` — used for stateful (that is, RNN-based) policies\n",
        "-   `info` — auxiliary data, such as log probabilities of actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5gCcpXswVAxk"
      },
      "outputs": [],
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D4DHZtq3Ndis"
      },
      "outputs": [],
      "source": [
        "time_step = example_environment.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PRFqAUzpNaAW"
      },
      "outputs": [],
      "source": [
        "# random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9bgU6Q6BZ8Bp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18.9"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "In order to keep track of the data collected from the environment, we will use [Reverb](https://deepmind.com/research/open-source/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data when we collect trajectories and is consumed during training.\n",
        "\n",
        "This replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpgdook5sr.\n",
            "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmpgdook5sr\n",
            "[reverb/cc/platform/default/server.cc:71] Started replay server on port 39373\n"
          ]
        }
      ],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_IZ-3HcqgE1z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
              "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
              "      dtype=float32)),\n",
              " 'policy_info': DictWrapper({'dist_params': DictWrapper({'logits': TensorSpec(shape=(2,), dtype=tf.float32, name='CategoricalProjectionNetwork_logits')})}),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sy6g1tGcfRlw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer.\n",
        "\n",
        "Here we are using 'PyDriver' to run the experience collecting loop. You can learn more about TF Agents driver in our [drivers tutorial](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TimeStep(\n",
              " {'discount': array(1., dtype=float32),\n",
              "  'observation': array([ 0.06820785,  0.93824077, -0.0328285 , -1.4877801 ], dtype=float32),\n",
              "  'reward': array(1., dtype=float32),\n",
              "  'step_type': array(1, dtype=int32)}),\n",
              " ())"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4wZnLu2ViO4E"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n",
              " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(64, 2, 4), dtype=tf.float32, name=None),\n",
              " 'policy_info': DictWrapper({'dist_params': DictWrapper({'logits': TensorSpec(shape=(64, 2, 2), dtype=tf.float32, name=None)})}),\n",
              " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "K13AST-2ppOq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3ccbf26910>\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Th5w5Sff0b16"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.train.utils import train_utils\n",
        "from tf_agents.train import actor\n",
        "\n",
        "train_step = train_utils.create_train_step()\n",
        "\n",
        "initial_collect_actor = actor.Actor(\n",
        "  train_py_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()\n",
        "\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.train import learner\n",
        "import os \n",
        "\n",
        "tempdir = \"/home/ramu/Personal/RL-Project/tempdir/\"\n",
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  train_py_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "('policy_info', 'dist_params', 'logits')",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/reverb/trajectory_writer.py:279\u001b[0m, in \u001b[0;36mTrajectoryWriter.append\u001b[0;34m(self, data, partial_step)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m   \u001b[39m# Use our custom mapping to flatten the expanded structure into columns.\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m   flat_column_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_column_order(data_with_path_flat)\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m   \u001b[39m# `data` contains fields which haven't been observed before so we need\u001b[39;00m\n\u001b[1;32m    282\u001b[0m   \u001b[39m# expand the spec using the union of the history and `data`.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/reverb/trajectory_writer.py:466\u001b[0m, in \u001b[0;36mTrajectoryWriter._apply_column_order\u001b[0;34m(self, data_with_path_flat)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mfor\u001b[39;00m path, value \u001b[39min\u001b[39;00m data_with_path_flat:\n\u001b[0;32m--> 466\u001b[0m   flat_data[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path_to_column_index[path]] \u001b[39m=\u001b[39m value\n\u001b[1;32m    467\u001b[0m \u001b[39mreturn\u001b[39;00m flat_data\n",
            "\u001b[0;31mKeyError\u001b[0m: ('policy_info', 'dist_params', 'logits')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb Cell 62\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m collect_driver \u001b[39m=\u001b[39m py_driver\u001b[39m.\u001b[39mPyDriver(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     env,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     py_tf_eager_policy\u001b[39m.\u001b[39mPyTFEagerPolicy(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m       agent\u001b[39m.\u001b[39mcollect_policy, use_tf_function\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     [rb_observer],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     max_steps\u001b[39m=\u001b[39mcollect_steps_per_iteration)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m   \u001b[39m# Collect a few steps and save to the replay buffer.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m   time_step, _ \u001b[39m=\u001b[39m collect_driver\u001b[39m.\u001b[39;49mrun(time_step)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#   # Sample a batch of data from the buffer and update the agent's network.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#   experience, unused_info = next(iterator)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#   train_loss = agent.train(experience).loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#     print('step = {0}: Average Return = {1}'.format(step, avg_return))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab/home/ramu/Personal/RL-Project/tutorials/dqn_tutorial.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#     returns.append(avg_return)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/tf_agents/drivers/py_driver.py:132\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    130\u001b[0m   observer((time_step, action_step_with_previous_state, next_time_step))\n\u001b[1;32m    131\u001b[0m \u001b[39mfor\u001b[39;00m observer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers:\n\u001b[0;32m--> 132\u001b[0m   observer(traj)\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m observer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo_observers:\n\u001b[1;32m    134\u001b[0m   observer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mget_info())\n",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/tf_agents/replay_buffers/reverb_utils.py:370\u001b[0m, in \u001b[0;36mReverbAddTrajectoryObserver.__call__\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Writes the trajectory into the underlying replay buffer.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[39mAllows trajectory to be a flattened trajectory. No batch dimension allowed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39m    there is *no* batch dimension.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_trajectory \u001b[39m=\u001b[39m trajectory\n\u001b[0;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writer\u001b[39m.\u001b[39;49mappend(trajectory)\n\u001b[1;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    373\u001b[0m \u001b[39m# If the fixed sequence length is reached, write the sequence.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/reverb/trajectory_writer.py:287\u001b[0m, in \u001b[0;36mTrajectoryWriter.append\u001b[0;34m(self, data, partial_step)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m   \u001b[39m# `data` contains fields which haven't been observed before so we need\u001b[39;00m\n\u001b[1;32m    282\u001b[0m   \u001b[39m# expand the spec using the union of the history and `data`.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_structure(\n\u001b[1;32m    284\u001b[0m       _tree_union(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure, tree\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m x: \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m                                                       data)))\n\u001b[0;32m--> 287\u001b[0m   flat_column_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_column_order(data_with_path_flat)\n\u001b[1;32m    289\u001b[0m \u001b[39m# If the last step is still open then verify that already populated columns\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m# are None in the new `data`.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_step_is_open:\n",
            "File \u001b[0;32m~/miniconda3/envs/rl_project3/lib/python3.11/site-packages/reverb/trajectory_writer.py:466\u001b[0m, in \u001b[0;36mTrajectoryWriter._apply_column_order\u001b[0;34m(self, data_with_path_flat)\u001b[0m\n\u001b[1;32m    464\u001b[0m flat_data \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_to_column_index)\n\u001b[1;32m    465\u001b[0m \u001b[39mfor\u001b[39;00m path, value \u001b[39min\u001b[39;00m data_with_path_flat:\n\u001b[0;32m--> 466\u001b[0m   flat_data[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path_to_column_index[path]] \u001b[39m=\u001b[39m value\n\u001b[1;32m    467\u001b[0m \u001b[39mreturn\u001b[39;00m flat_data\n",
            "\u001b[0;31mKeyError\u001b[0m: ('policy_info', 'dist_params', 'logits')"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# # Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# # Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# # Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# # Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "#   # Sample a batch of data from the buffer and update the agent's network.\n",
        "#   experience, unused_info = next(iterator)\n",
        "#   train_loss = agent.train(experience).loss\n",
        "\n",
        "#   step = agent.train_step_counter.numpy()\n",
        "\n",
        "#   if step % log_interval == 0:\n",
        "#     print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "#   if step % eval_interval == 0:\n",
        "#     avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "#     print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "#     returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
        "\n",
        "First, create a function to embed videos in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povaAOcZygLw"
      },
      "source": [
        "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "outputs": [],
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
