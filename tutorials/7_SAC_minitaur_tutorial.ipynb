{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGNgWREsvQv"
   },
   "source": [
    "**Copyright 2023 The TF-Agents Authors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:02.043165Z",
     "iopub.status.busy": "2023-09-26T11:16:02.042943Z",
     "iopub.status.idle": "2023-09-26T11:16:02.046648Z",
     "shell.execute_reply": "2023-09-26T11:16:02.046085Z"
    },
    "id": "nQnmcm0oI1Q-"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "# SAC minitaur with the Actor-Learner API\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOUOQOrFs3zn"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKOCZlhUgXVK"
   },
   "source": [
    "This example shows how to train a [Soft Actor Critic](https://arxiv.org/abs/1812.05905) agent on the [Minitaur](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py) environment.\n",
    "\n",
    "If you've worked through the [DQN Colab](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb) this should feel very familiar. Notable changes include:\n",
    "\n",
    "  * Changing the agent from DQN to SAC.\n",
    "  * Training on Minitaur which is a much more complex environment than CartPole. The Minitaur environment aims to train a quadruped robot to move forward.\n",
    "  * Using the TF-Agents Actor-Learner API for distributed Reinforcement Learning.\n",
    "\n",
    "The API supports both distributed data collection using an experience replay buffer and variable container (parameter server) and distributed training across multiple devices. The API is designed to be very simple and modular. We utilize [Reverb](https://deepmind.com/research/open-source/Reverb) for both replay buffer and variable container and [TF DistributionStrategy API](https://www.tensorflow.org/guide/distributed_training) for distributed training on GPUs and TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vUQms4DAY5A"
   },
   "source": [
    "If you haven't installed the following dependencies, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:02.050427Z",
     "iopub.status.busy": "2023-09-26T11:16:02.050011Z",
     "iopub.status.idle": "2023-09-26T11:16:51.290916Z",
     "shell.execute_reply": "2023-09-26T11:16:51.289890Z"
    },
    "id": "fskoLlB-AZ9j"
   },
   "outputs": [],
   "source": [
    "# !sudo apt-get update\n",
    "# !sudo apt-get install -y xvfb ffmpeg\n",
    "# !pip install 'imageio==2.4.0'\n",
    "# !pip install matplotlib\n",
    "# !pip install tf-agents[reverb]\n",
    "# !pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNV5wyH3dyMl"
   },
   "source": [
    "First we will import the different tools that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:51.295398Z",
     "iopub.status.busy": "2023-09-26T11:16:51.295125Z",
     "iopub.status.idle": "2023-09-26T11:16:55.216819Z",
     "shell.execute_reply": "2023-09-26T11:16:55.215923Z"
    },
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import reverb\n",
    "import tempfile\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.environments import suite_pybullet\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "tempdir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmC0NDhdLIKY"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:55.221080Z",
     "iopub.status.busy": "2023-09-26T11:16:55.220379Z",
     "iopub.status.idle": "2023-09-26T11:16:55.225676Z",
     "shell.execute_reply": "2023-09-26T11:16:55.224972Z"
    },
    "id": "HC1kNrOsLSIZ"
   },
   "outputs": [],
   "source": [
    "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
    "# env_name = \"CartPole-v0\"\n",
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 100000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Environment\n",
    "\n",
    "Environments in RL represent the task or problem that we are trying to solve. Standard environments can be easily created in TF-Agents using `suites`. We have different `suites` for loading environments from sources such as the OpenAI Gym, Atari, DM Control, etc., given a string environment name.\n",
    "\n",
    "Now let's load the Minitaur environment from the Pybullet suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:55.229017Z",
     "iopub.status.busy": "2023-09-26T11:16:55.228515Z",
     "iopub.status.idle": "2023-09-26T11:16:55.912373Z",
     "shell.execute_reply": "2023-09-26T11:16:55.911682Z"
    },
    "id": "RlO7WIQHu_7D"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = suite_pybullet.load(env_name)\n",
    "env.reset()\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY179d1xlmoM"
   },
   "source": [
    "In this environment the goal is for the agent to train a policy that will control the Minitaur robot and have it move forward as fast as possible. Episodes last 1000 steps and the return will be the sum of rewards throughout the episode.\n",
    "\n",
    "Let's look at the information the environment provides as an `observation` which the policy will use to generate `actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:55.916843Z",
     "iopub.status.busy": "2023-09-26T11:16:55.916591Z",
     "iopub.status.idle": "2023-09-26T11:16:55.923261Z",
     "shell.execute_reply": "2023-09-26T11:16:55.922668Z"
    },
    "id": "exDv57iHfwQV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg5ysVTnctIm"
   },
   "source": [
    "The observation is fairly complex. We receive 28 values representing the angles, velocities, and torques for all the motors. In return the environment expects 8 values for the actions between `[-1, 1]`. These are the desired motor angles.\n",
    "\n",
    "Usually we create two environments: one for collecting data during training and one for evaluation. The environments are written in pure python and use numpy arrays, which the Actor Learner API directly consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:55.926703Z",
     "iopub.status.busy": "2023-09-26T11:16:55.926471Z",
     "iopub.status.idle": "2023-09-26T11:16:56.258399Z",
     "shell.execute_reply": "2023-09-26T11:16:56.257702Z"
    },
    "id": "Xp-Y4mD6eDhF"
   },
   "outputs": [],
   "source": [
    "collect_env = suite_pybullet.load(env_name)\n",
    "eval_env = suite_pybullet.load(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da-z2yF66FR9"
   },
   "source": [
    "## Distribution Strategy\n",
    "We use the DistributionStrategy API to enable running the train step computation across multiple devices such as multiple GPUs or TPUs using data parallelism. The train step:\n",
    "* Receives a batch of training data\n",
    "* Splits it across the devices\n",
    "* Computes the forward step\n",
    "* Aggregates and computes the MEAN of the loss\n",
    "* Computes the backward step and performs a gradient variable update\n",
    "\n",
    "With TF-Agents Learner API and DistributionStrategy API it is quite easy to switch between running the train step on GPUs (using MirroredStrategy) to TPUs (using TPUStrategy) without changing any of the training logic below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGREYZCaDB1h"
   },
   "source": [
    "### Enabling the GPU\n",
    "If you want to try running on a GPU, you'll first need to enable GPUs for the notebook:\n",
    "\n",
    "* Navigate to Edit→Notebook Settings\n",
    "* Select GPU from the Hardware Accelerator drop-down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZuvwDV66Mn1"
   },
   "source": [
    "### Picking a strategy\n",
    "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
    "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
    "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:56.262018Z",
     "iopub.status.busy": "2023-09-26T11:16:56.261746Z",
     "iopub.status.idle": "2023-09-26T11:16:56.929034Z",
     "shell.execute_reply": "2023-09-26T11:16:56.928180Z"
    },
    "id": "ff5ZZRZI15ds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True #@param {type:\"boolean\"}\n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMn5FTs5kHvt"
   },
   "source": [
    "All variables and Agents need to be created under `strategy.scope()`, as you'll see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9lW_OZYFR8A"
   },
   "source": [
    "## Agent\n",
    "\n",
    "To create an SAC Agent, we first need to create the networks that it will train. SAC is an actor-critic agent, so we will need two networks.\n",
    "\n",
    "The critic will give us value estimates for `Q(s,a)`. That is, it will recieve as input an observation and an action, and it will give us an estimate of how good that action was for the given state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:56.932978Z",
     "iopub.status.busy": "2023-09-26T11:16:56.932715Z",
     "iopub.status.idle": "2023-09-26T11:16:56.956161Z",
     "shell.execute_reply": "2023-09-26T11:16:56.955474Z"
    },
    "id": "TgkdEPg_muzV"
   },
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(collect_env))\n",
    "\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYy4AH4V7Ph4"
   },
   "source": [
    "We will use this critic to train an `actor` network which will allow us to generate actions given an observation.\n",
    "\n",
    "The `ActorNetwork` will predict parameters for a tanh-squashed [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) distribution. This distribution will then be sampled, conditioned on the current observation, whenever we need to generate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:56.959627Z",
     "iopub.status.busy": "2023-09-26T11:16:56.959355Z",
     "iopub.status.idle": "2023-09-26T11:16:56.988297Z",
     "shell.execute_reply": "2023-09-26T11:16:56.987659Z"
    },
    "id": "TB5Y3Oub4u7f"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=(\n",
    "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z62u55hSmviJ"
   },
   "source": [
    "With these networks at hand we can now instantiate the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:56.991909Z",
     "iopub.status.busy": "2023-09-26T11:16:56.991338Z",
     "iopub.status.idle": "2023-09-26T11:16:57.196028Z",
     "shell.execute_reply": "2023-09-26T11:16:57.195279Z"
    },
    "id": "jbY4yrjTEyc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ppo import ppo_agent\n",
    "with strategy.scope():\n",
    "    train_step = train_utils.create_train_step()\n",
    "\n",
    "#   tf_agent = sac_agent.SacAgent(\n",
    "#         time_step_spec,\n",
    "#         action_spec,\n",
    "#         actor_network=actor_net,\n",
    "#         critic_network=critic_net,\n",
    "#         actor_optimizer=tf.keras.optimizers.Adam(\n",
    "#             learning_rate=actor_learning_rate),\n",
    "#         critic_optimizer=tf.keras.optimizers.Adam(\n",
    "#             learning_rate=critic_learning_rate),\n",
    "#         alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "#             learning_rate=alpha_learning_rate),\n",
    "#         target_update_tau=target_update_tau,\n",
    "#         target_update_period=target_update_period,\n",
    "#         td_errors_loss_fn=tf.math.squared_difference,\n",
    "#         gamma=gamma,\n",
    "#         reward_scale_factor=reward_scale_factor,\n",
    "#         train_step_counter=train_step)\n",
    "    tf_agent = ppo_agent.PPOAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_net=actor_net,\n",
    "        value_net=critic_net,\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=actor_learning_rate),\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "    tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "In order to keep track of the data collected from the environment, we will use [Reverb](https://deepmind.com/research/open-source/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data collected by the Actors and consumed by the Learner during training.\n",
    "\n",
    "In this tutorial, this is less important than `max_size` -- but in a distributed setting with async collection and training, you will probably want to experiment with `rate_limiters.SampleToInsertRatio`, using a samples_per_insert somewhere between 2 and 1000. For example:\n",
    "```\n",
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.200148Z",
     "iopub.status.busy": "2023-09-26T11:16:57.199538Z",
     "iopub.status.idle": "2023-09-26T11:16:57.209107Z",
     "shell.execute_reply": "2023-09-26T11:16:57.208434Z"
    },
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmplqnqcea4.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmplqnqcea4\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 42593\n"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRNvAnkO7JK2"
   },
   "source": [
    "The replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using `tf_agent.collect_data_spec`.\n",
    "\n",
    "Since the SAC Agent needs both the current and next observation to compute the loss, we set `sequence_length=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.212163Z",
     "iopub.status.busy": "2023-09-26T11:16:57.211824Z",
     "iopub.status.idle": "2023-09-26T11:16:57.217944Z",
     "shell.execute_reply": "2023-09-26T11:16:57.217325Z"
    },
    "id": "xVLUxyUo7HQR"
   },
   "outputs": [],
   "source": [
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVD5nQ9ZGo8_"
   },
   "source": [
    "Now we generate a TensorFlow dataset from the Reverb replay buffer. We will pass this to the Learner to sample experiences for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.221071Z",
     "iopub.status.busy": "2023-09-26T11:16:57.220820Z",
     "iopub.status.idle": "2023-09-26T11:16:57.544325Z",
     "shell.execute_reply": "2023-09-26T11:16:57.543631Z"
    },
    "id": "ba7bilizt_qW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/default/server.cc:84] Shutting down replay server\n"
     ]
    }
   ],
   "source": [
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0KLrEPwkn5x"
   },
   "source": [
    "## Policies\n",
    "\n",
    "In TF-Agents, policies represent the standard notion of policies in RL: given a `time_step` produce an action or a distribution over actions. The main method is `policy_step = policy.step(time_step)` where `policy_step` is a named tuple `PolicyStep(action, state, info)`.  The `policy_step.action` is the `action` to be applied to the environment, `state` represents the state for stateful (RNN) policies and `info` may contain auxiliary information such as log probabilities of the actions.\n",
    "\n",
    "Agents contain two policies:\n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.548357Z",
     "iopub.status.busy": "2023-09-26T11:16:57.547883Z",
     "iopub.status.idle": "2023-09-26T11:16:57.552436Z",
     "shell.execute_reply": "2023-09-26T11:16:57.551847Z"
    },
    "id": "yq7JE8IwFe0E"
   },
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.555731Z",
     "iopub.status.busy": "2023-09-26T11:16:57.555180Z",
     "iopub.status.idle": "2023-09-26T11:16:57.559270Z",
     "shell.execute_reply": "2023-09-26T11:16:57.558686Z"
    },
    "id": "f_A4rZveEQzW"
   },
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azkJZ8oaF8uc"
   },
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_py_policy` to create a policy which will randomly select an action for each time_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.562303Z",
     "iopub.status.busy": "2023-09-26T11:16:57.562079Z",
     "iopub.status.idle": "2023-09-26T11:16:57.565809Z",
     "shell.execute_reply": "2023-09-26T11:16:57.565248Z"
    },
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  collect_env.time_step_spec(), collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1LMqw60Kuso"
   },
   "source": [
    "## Actors\n",
    "The actor manages interactions between a policy and an environment.\n",
    "  * The Actor components contain an instance of the environment (as `py_environment`) and a copy of the policy variables.\n",
    "  * Each Actor worker runs a sequence of data collection steps given the local values of the policy variables.\n",
    "  * Variable updates are done explicitly using the variable container client instance in the training script before calling `actor.run()`.\n",
    "  * The observed experience is written into the replay buffer in each data collection step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjE59ct9fU7W"
   },
   "source": [
    "As the Actors run data collection steps, they pass trajectories of (state, action, reward) to the observer, which caches and writes them to the Reverb replay system. \n",
    "\n",
    "We're storing trajectories for frames [(t0,t1) (t1,t2) (t2,t3), ...] because `stride_length=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.568924Z",
     "iopub.status.busy": "2023-09-26T11:16:57.568687Z",
     "iopub.status.idle": "2023-09-26T11:16:57.572141Z",
     "shell.execute_reply": "2023-09-26T11:16:57.571548Z"
    },
    "id": "HbyGmdiNfNDc"
   },
   "outputs": [],
   "source": [
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2,\n",
    "  stride_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yaVVC22fOcF"
   },
   "source": [
    "We create an Actor with the random policy and collect experiences to seed the replay buffer with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:16:57.575123Z",
     "iopub.status.busy": "2023-09-26T11:16:57.574877Z",
     "iopub.status.idle": "2023-09-26T11:18:04.876863Z",
     "shell.execute_reply": "2023-09-26T11:18:04.875898Z"
    },
    "id": "ZGq3SY0kKwsa"
   },
   "outputs": [],
   "source": [
    "initial_collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=[rb_observer])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pkg-0vZP_Ya"
   },
   "source": [
    "Instantiate an Actor with the collect policy to gather more experiences during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:04.881213Z",
     "iopub.status.busy": "2023-09-26T11:18:04.880575Z",
     "iopub.status.idle": "2023-09-26T11:18:05.021340Z",
     "shell.execute_reply": "2023-09-26T11:18:05.020639Z"
    },
    "id": "A6ooXyk0FZ5j"
   },
   "outputs": [],
   "source": [
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run=1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FR9CZ-jfPN2T"
   },
   "source": [
    "Create an Actor which will be used to evaluate the policy during training. We pass in `actor.eval_metrics(num_eval_episodes)` to log metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:05.025292Z",
     "iopub.status.busy": "2023-09-26T11:18:05.024601Z",
     "iopub.status.idle": "2023-09-26T11:18:05.159003Z",
     "shell.execute_reply": "2023-09-26T11:18:05.158381Z"
    },
    "id": "vHY2BT5lFhgL"
   },
   "outputs": [],
   "source": [
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6eBGSYiOf83"
   },
   "source": [
    "## Learners\n",
    "The Learner component contains the agent and performs gradient step updates to the policy variables using experience data from the replay buffer. After one or more training steps, the Learner can push a new set of variable values to the variable container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:05.162647Z",
     "iopub.status.busy": "2023-09-26T11:18:05.162212Z",
     "iopub.status.idle": "2023-09-26T11:18:13.124914Z",
     "shell.execute_reply": "2023-09-26T11:18:13.124062Z"
    },
    "id": "gi37YicSFTfF"
   },
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "saved_model_dir = f\"/home/ramu/Personal/RL-Project/dump/{learner.POLICY_SAVED_MODEL_DIR}\"\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers,\n",
    "  strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "We instantiated the eval Actor with `actor.eval_metrics` above, which creates most commonly used metrics during policy evaluation:\n",
    "* Average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes.\n",
    "* Average episode length.\n",
    "\n",
    "We run the Actor to generate these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:13.129247Z",
     "iopub.status.busy": "2023-09-26T11:18:13.128586Z",
     "iopub.status.idle": "2023-09-26T11:18:21.567133Z",
     "shell.execute_reply": "2023-09-26T11:18:21.566359Z"
    },
    "id": "83iMSHUC71RG"
   },
   "outputs": [],
   "source": [
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:21.571379Z",
     "iopub.status.busy": "2023-09-26T11:18:21.570658Z",
     "iopub.status.idle": "2023-09-26T11:18:21.575349Z",
     "shell.execute_reply": "2023-09-26T11:18:21.574717Z"
    },
    "id": "jnOMvX_eZvOW"
   },
   "outputs": [],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWWURm_rXG-f"
   },
   "source": [
    "Check out the [metrics module](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py) for other standard implementations of different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "The training loop involves both collecting data from the environment and optimizing the agent's networks. Along the way, we will occasionally evaluate the agent's policy to see how we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:18:21.578654Z",
     "iopub.status.busy": "2023-09-26T11:18:21.578436Z",
     "iopub.status.idle": "2023-09-26T11:56:36.970085Z",
     "shell.execute_reply": "2023-09-26T11:56:36.969329Z"
    },
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "We can plot average return vs global steps to see the performance of our agent. In `Minitaur`, the reward function is based on how far the minitaur walks in 1000 steps and penalizes the energy expenditure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:56:36.975014Z",
     "iopub.status.busy": "2023-09-26T11:56:36.974484Z",
     "iopub.status.idle": "2023-09-26T11:56:37.130207Z",
     "shell.execute_reply": "2023-09-26T11:56:37.129546Z"
    },
    "id": "rXKzyGt72HS8"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "It is helpful to visualize the performance of an agent by rendering the environment at each step. Before we do that, let us first create a function to embed videos in this colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:56:37.133636Z",
     "iopub.status.busy": "2023-09-26T11:56:37.133375Z",
     "iopub.status.idle": "2023-09-26T11:56:37.137787Z",
     "shell.execute_reply": "2023-09-26T11:56:37.137151Z"
    },
    "id": "ULaGr8pvOKbl"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "The following code visualizes the agent's policy for a few episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T11:56:37.141042Z",
     "iopub.status.busy": "2023-09-26T11:56:37.140391Z",
     "iopub.status.idle": "2023-09-26T11:58:19.184158Z",
     "shell.execute_reply": "2023-09-26T11:58:19.183103Z"
    },
    "id": "PSgaQN1nXT-h"
   },
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "video_filename = 'sac_minitaur.mp4'\n",
    "with imageio.get_writer(video_filename, fps=60) as video:\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = eval_env.reset()\n",
    "    video.append_data(eval_env.render())\n",
    "    while not time_step.is_last():\n",
    "      action_step = eval_actor.policy.action(time_step)\n",
    "      time_step = eval_env.step(action_step.action)\n",
    "      video.append_data(eval_env.render())\n",
    "\n",
    "embed_mp4(video_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_SAC_minitaur_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
